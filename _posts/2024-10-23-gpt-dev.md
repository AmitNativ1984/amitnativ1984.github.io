---
layout: posts
title:  "Build GPT from Scratch"
tagline: "Understanding the Attention Mechanism and Transformers Architecture"
date:   2024-10-23
categories: COMPUTER-VISION
permalink: /:categories/:title
toc: true
header:
    overlay_image: /assets/images/gpt/attention_heatmap.png
    overlay_filter: 0.5
created: true
classes: wide
---


# CODING GPT FROM SCRATCH

This Notebook follows [Code GPT From Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY), by Andrej Karpathy. This is my implementation, with my explanations, to better understand the way transformers work. 

The goal of this notebook is to implement a ***G***enrative ***P***retrained ***T***ransformer ***(GPT)***, that will generate Shakespeare like text.

## 1. Preparing the Dataset
In this projects, we are going to use [Tiny Shakespeare Dataset](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).

To prepare the dataset for processing, 

1. Download shakespeare data
2. Create tokenizer - what are the distinct charerchters we have?
3. create the enconder -> charecter to int
   create the decoder -> int back to charerechter.
4. show the first 1000 tokens in the dataset.
5. define block size
   define batch size
6. set the torch.seed to 1337 torch.seed(1037)
7. split to 90%-10% train/val
8. randomly sample the data.
   Create the batch size
   
   
 1. The initial loss should be that of a uniformal random variables:
    loss_i = -ln(1/65) [65 charecters/tokens] 

### 1.1 Download the data


```python
#!wget(https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()
```

### 1.2 Build the Tokenizer, Encoder and Decoder
Our goal is to have the ability to convert a string of text, into a sequence of integers, and back.
To do this, we first need to create a tokenizer, that will map each character to an integer. This is done by going over the entire dataset, and building the vocabulary: the set of all unique characters in the dataset.

After that, we will create the encoder - that maps a string of text into a sequence of numbers, and the decoder - that maps a sequence of numbers bak to a string of text.


```python
chars = sorted(list(set(text)))
vocab_size = len(chars)
print('Vocabulary size (num of unique chars):', vocab_size)

# create a mapping from character to index and vice versa
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}

# create the encoder (ch -> idx) and decoder (idx -> ch) functions
encode = lambda s: [stoi[c] for c in s] #encoder: take a string, output a list of integers
decode = lambda l: ''.join([itos[i] for i in l]) #decoder: take a list of integers, output a string

print(encode("hi there"))
print (decode(encode("hi there")))
```

    Vocabulary size (num of unique chars): 65
    [46, 47, 1, 58, 46, 43, 56, 43]
    hi there


### 1.3 Train and Test Split


```python
import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

```

    /opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
      return torch._C._cuda_getDeviceCount() > 0



```python

data = torch.tensor(encode(text), dtype=torch.long)
n = int(len(data)*0.9)
train_data, val_data = data[:n], data[n:]

# lets print the first 200 elements of data. This is what our data looks like.
print(data[:200])
```

    tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,
            53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,
             1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,
            57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,
             6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,
            58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,
             1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,
            53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,
            57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,
             8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,
             1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,
            53, 59])


## 2. Feeding Data Into The Network

We will never feed the entire dataset into the network. Instead we feed it with a sequence, up to a fixed block size. 

### 2.1. The Idea of Input `Context`, and `Block_Size` 
We are going to be using `B-Gram Model`: A very simple model, that predicts the `next 1 word` based on a given sequence. The input sequence is called the `context`. The maximum length `context`us called `block_size`. We want the model predict the next word, given `context` of length `[1..block_size]`. Later on, this will be usefull for the transformer network to predict sequences of any given length up to `block_size`


```python
block_size = 8
print(train_data[:block_size+1])
```

    tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])





```python
x = train_data[:block_size]
y = train_data[1:block_size+1]
print(f"The input tensor is: {x}")
print(f"The target tensor is: {y}")
print("So that...")
for t in range(block_size):
    context = x[:t+1]
    target =  y[t]
    print(f"For input {context}, the output is: {target}")
```

    The input tensor is: tensor([18, 47, 56, 57, 58,  1, 15, 47])
    The target tensor is: tensor([47, 56, 57, 58,  1, 15, 47, 58])
    So that...
    For input tensor([18]), the output is: 47
    For input tensor([18, 47]), the output is: 56
    For input tensor([18, 47, 56]), the output is: 57
    For input tensor([18, 47, 56, 57]), the output is: 58
    For input tensor([18, 47, 56, 57, 58]), the output is: 1
    For input tensor([18, 47, 56, 57, 58,  1]), the output is: 15
    For input tensor([18, 47, 56, 57, 58,  1, 15]), the output is: 47
    For input tensor([18, 47, 56, 57, 58,  1, 15, 47]), the output is: 58


## 2.2. Processing Input Batch

When `context` size is `[1,...,block_size]`; `block_size=8` We have `8` different input contexts, with corresponding `8` different output targets. In other words, `8` training examples. To improve training speed, we will be adding the `batch` dimension. We will be randomly sampling `batch_size` samples from the training data, and use them for training.


```python
torch.manual_seed(1337)
batch_size = 4
block_size = 8

def get_batch(split):
    data = train_data if split=='train' else val_data
    # Sample batch_size random samples from data
    ix = torch.randint(len(data) - block_size, (batch_size, ))
    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)
    y = torch.stack([data[i+1: i+ 1 + block_size] for i in ix]).to(device)
    
    return x, y
```


```python
xb, yb = get_batch(split="train")

print("input batch:")
print(f"shape: {xb.shape}")
print(xb)

print("target batch:")
print(f"shape: {yb.shape}")
print(yb)
```

    input batch:
    shape: torch.Size([4, 8])
    tensor([[56,  6,  0, 24, 43, 58,  1, 61],
            [39, 47, 51,  1, 58, 46, 39, 58],
            [52, 45,  1, 58, 53,  1, 57, 39],
            [43, 47, 52, 45,  1, 46, 53, 50]])
    target batch:
    shape: torch.Size([4, 8])
    tensor([[ 6,  0, 24, 43, 58,  1, 61, 46],
            [47, 51,  1, 58, 46, 39, 58,  1],
            [45,  1, 58, 53,  1, 57, 39, 63],
            [47, 52, 45,  1, 46, 53, 50, 47]])


There are 4 batches. 
Each batch has 8 different contexts (`1,..,block_size`).
There are 8*4=32 different input-target pairs in every batch.


```python
for b in range(batch_size): # batch dimension
    for t in range(block_size): # time dimension
        context = xb[b, :t+1]
        target = yb[b, t]
        print(f"For input tensor: {context}, the target is: {target}")
    print ("-----")
```

    For input tensor: tensor([56]), the target is: 6
    For input tensor: tensor([56,  6]), the target is: 0
    For input tensor: tensor([56,  6,  0]), the target is: 24
    For input tensor: tensor([56,  6,  0, 24]), the target is: 43
    For input tensor: tensor([56,  6,  0, 24, 43]), the target is: 58
    For input tensor: tensor([56,  6,  0, 24, 43, 58]), the target is: 1
    For input tensor: tensor([56,  6,  0, 24, 43, 58,  1]), the target is: 61
    For input tensor: tensor([56,  6,  0, 24, 43, 58,  1, 61]), the target is: 46
    -----
    For input tensor: tensor([39]), the target is: 47
    For input tensor: tensor([39, 47]), the target is: 51
    For input tensor: tensor([39, 47, 51]), the target is: 1
    For input tensor: tensor([39, 47, 51,  1]), the target is: 58
    For input tensor: tensor([39, 47, 51,  1, 58]), the target is: 46
    For input tensor: tensor([39, 47, 51,  1, 58, 46]), the target is: 39
    For input tensor: tensor([39, 47, 51,  1, 58, 46, 39]), the target is: 58
    For input tensor: tensor([39, 47, 51,  1, 58, 46, 39, 58]), the target is: 1
    -----
    For input tensor: tensor([52]), the target is: 45
    For input tensor: tensor([52, 45]), the target is: 1
    For input tensor: tensor([52, 45,  1]), the target is: 58
    For input tensor: tensor([52, 45,  1, 58]), the target is: 53
    For input tensor: tensor([52, 45,  1, 58, 53]), the target is: 1
    For input tensor: tensor([52, 45,  1, 58, 53,  1]), the target is: 57
    For input tensor: tensor([52, 45,  1, 58, 53,  1, 57]), the target is: 39
    For input tensor: tensor([52, 45,  1, 58, 53,  1, 57, 39]), the target is: 63
    -----
    For input tensor: tensor([43]), the target is: 47
    For input tensor: tensor([43, 47]), the target is: 52
    For input tensor: tensor([43, 47, 52]), the target is: 45
    For input tensor: tensor([43, 47, 52, 45]), the target is: 1
    For input tensor: tensor([43, 47, 52, 45,  1]), the target is: 46
    For input tensor: tensor([43, 47, 52, 45,  1, 46]), the target is: 53
    For input tensor: tensor([43, 47, 52, 45,  1, 46, 53]), the target is: 50
    For input tensor: tensor([43, 47, 52, 45,  1, 46, 53, 50]), the target is: 47
    -----


## 3. Building the B-Gram Langunage Model

As described before, this is a very simple language model. The target is to predict the next 1st word based on a given sequence.


```python
class BigramLanguageModel(nn.Module):
    """ Bigram Language Model.
        Args:
            vocab_size (int): Number of tokens (size of vocabulary)
    """

    def __init__(self, vocab_size):
        super().__init__()
        # Each token directiry reads off the logits for the next token from a lookup table
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def forward(self, idx, targets=None):
        logits = self.token_embedding_table(idx) # B,T,C
        # cross_entropy expects the input (logits) to be of shape (B, C, T)
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            loss = F.cross_entropy(logits.view(B*T, C),  targets.view(B*T))

        return logits ,loss

    def generate(self, idx, max_new_tokens):
        """ Generate new tokens, given current context.
            In a simple B-gram model, we just need to look up the logits for the next token.
            In more complex models, we are also intrested in the history of the tokens. In other
            words, a larger context.

            Args:
                idx (Tensor): Initial context
                max_new_tokens (int): Maximum number of new tokens to generate.
            Returns:
                Tensor: Generated tokens

        """

        # idx is a tensor of shape (B, T)
        idx.to(device)
        for i in range(max_new_tokens):
            # get the prediction for the current context
            logits, loss = self.forward(idx) # logits shape B, T, C
            # In B-gram model, focus only on the last time step.
            logits = logits[:, -1, :]   
            # convert logits to probabilities
            probs = F.softmax(logits, dim=-1)
            # sample the next token from the distribution
            next_token = torch.multinomial(probs, num_samples=1).to(device)
            idx = torch.cat([idx, next_token], dim=1) # shape B, T+1

        return idx
            
            


model = BigramLanguageModel(vocab_size)
model.to(device)
out, loss = model(xb, yb)
print(out.shape)
print(loss)

# We expect the loss to be around -log(1/vocab_size) = -log(1/65) = 4.17
print(f"Expected loss for uniform distribution: {-torch.log(torch.tensor(1/vocab_size))}")

# Example - generate new tokens
print("\n")
print("Example - generate new tokens")
print("(Model is not trained yet, so it will generate random tokens)")
idx = torch.zeros((1,1), dtype=torch.long).to(device)
new_tokens = model.generate(idx, 100)[0].tolist()

# decode the generated tokens
gen_txt = decode(new_tokens)
print(gen_txt)

```

    torch.Size([4, 8, 65])
    tensor(4.4609, grad_fn=<NllLossBackward0>)
    Expected loss for uniform distribution: 4.174387454986572
    
    
    Example - generate new tokens
    (Model is not trained yet, so it will generate random tokens)
    
    l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV
    vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq


### 3.1 Training the B-Gram Model
Without training the model, the results are random. The loss is roughly `-ln(1/vocab_size)`. To improve the model, we will train it with `AdamW` optimizer, and implement a simple training loop. 


```python
# define the optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
```

The code block below, is used to estimate the loss. Instead of logging the loss of every epoch - which is very noisy, it will average the loss over `eval_iters` epochs. We shall run this block on both training and validation data. While training, it is important to see that the training and validation loss are decreasing. If the validation loss is increasing, it is a sign of overfitting.


```python
eval_iters = 200
@torch.no_grad()
def estimate_loss():
    # estimate the loss on training and validation sets
    out = {}
    losses = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters).to(device)
        for k in range(eval_iters):
            xb, yb = get_batch(split)
            logits, loss = model(xb, yb)
            losses[k] = loss.item()
        out[split] = losses.mean().item()
    model.train()
    return out
        
```


```python
# define the training loop
num_epochs = 10000
batch_size = 32
eval_interval = 100

model = model.to(device)
for step in range(num_epochs):
    
    if step % eval_interval == 0:
        losses = estimate_loss()
        print(f"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}")

    # sample a batch of data
    xb, yb = get_batch(split="train")
    # forward pass
    logits, loss = model(xb, yb)

    # backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(f"Loss = {loss.item()}") # this prints the loss of the last batch
```

    Step: 0, Train Loss: 4.635080337524414, Val Loss: 4.645778656005859
    Step: 100, Train Loss: 4.517083644866943, Val Loss: 4.5349040031433105
    Step: 200, Train Loss: 4.418772220611572, Val Loss: 4.426044940948486
    Step: 300, Train Loss: 4.308123588562012, Val Loss: 4.3238372802734375
    Step: 400, Train Loss: 4.209145545959473, Val Loss: 4.215800762176514
    Step: 500, Train Loss: 4.1152520179748535, Val Loss: 4.1276350021362305
    Step: 600, Train Loss: 4.03169059753418, Val Loss: 4.0318074226379395
    Step: 700, Train Loss: 3.9412500858306885, Val Loss: 3.950608015060425
    Step: 800, Train Loss: 3.849108934402466, Val Loss: 3.866924524307251
    Step: 900, Train Loss: 3.7737972736358643, Val Loss: 3.7728090286254883
    Step: 1000, Train Loss: 3.695526123046875, Val Loss: 3.7079856395721436
    Step: 1100, Train Loss: 3.6236352920532227, Val Loss: 3.636014699935913
    Step: 1200, Train Loss: 3.548992872238159, Val Loss: 3.570427894592285
    Step: 1300, Train Loss: 3.486809730529785, Val Loss: 3.5061960220336914
    Step: 1400, Train Loss: 3.432687997817993, Val Loss: 3.443000078201294
    Step: 1500, Train Loss: 3.365821123123169, Val Loss: 3.3859517574310303
    Step: 1600, Train Loss: 3.3122241497039795, Val Loss: 3.331522226333618
    Step: 1700, Train Loss: 3.267331838607788, Val Loss: 3.2777023315429688
    Step: 1800, Train Loss: 3.214113473892212, Val Loss: 3.235337018966675
    Step: 1900, Train Loss: 3.164638042449951, Val Loss: 3.179840087890625
    Step: 2000, Train Loss: 3.1184422969818115, Val Loss: 3.1411118507385254
    Step: 2100, Train Loss: 3.0733537673950195, Val Loss: 3.0890612602233887
    Step: 2200, Train Loss: 3.0405967235565186, Val Loss: 3.0562403202056885
    Step: 2300, Train Loss: 3.0060110092163086, Val Loss: 3.0216848850250244
    Step: 2400, Train Loss: 2.9695000648498535, Val Loss: 2.979281425476074
    Step: 2500, Train Loss: 2.9345510005950928, Val Loss: 2.962341547012329
    Step: 2600, Train Loss: 2.9078528881073, Val Loss: 2.925280809402466
    Step: 2700, Train Loss: 2.874791145324707, Val Loss: 2.8981809616088867
    Step: 2800, Train Loss: 2.8528330326080322, Val Loss: 2.8556103706359863
    Step: 2900, Train Loss: 2.825302839279175, Val Loss: 2.8401708602905273
    Step: 3000, Train Loss: 2.7974953651428223, Val Loss: 2.817849636077881
    Step: 3100, Train Loss: 2.7818894386291504, Val Loss: 2.798487663269043
    Step: 3200, Train Loss: 2.754465103149414, Val Loss: 2.7901172637939453
    Step: 3300, Train Loss: 2.741487741470337, Val Loss: 2.7638895511627197
    Step: 3400, Train Loss: 2.7283074855804443, Val Loss: 2.7388198375701904
    Step: 3500, Train Loss: 2.716320753097534, Val Loss: 2.730480432510376
    Step: 3600, Train Loss: 2.691248893737793, Val Loss: 2.703120708465576
    Step: 3700, Train Loss: 2.6780645847320557, Val Loss: 2.6961379051208496
    Step: 3800, Train Loss: 2.6637914180755615, Val Loss: 2.6769280433654785
    Step: 3900, Train Loss: 2.651124954223633, Val Loss: 2.6774957180023193
    Step: 4000, Train Loss: 2.638995409011841, Val Loss: 2.6604833602905273
    Step: 4100, Train Loss: 2.6206014156341553, Val Loss: 2.6506400108337402
    Step: 4200, Train Loss: 2.619706630706787, Val Loss: 2.6411848068237305
    Step: 4300, Train Loss: 2.6124842166900635, Val Loss: 2.6185741424560547
    Step: 4400, Train Loss: 2.598726511001587, Val Loss: 2.6216115951538086
    Step: 4500, Train Loss: 2.5970380306243896, Val Loss: 2.611767053604126
    Step: 4600, Train Loss: 2.58583927154541, Val Loss: 2.6018545627593994
    Step: 4700, Train Loss: 2.5735912322998047, Val Loss: 2.598369836807251
    Step: 4800, Train Loss: 2.569993257522583, Val Loss: 2.599987745285034
    Step: 4900, Train Loss: 2.5702714920043945, Val Loss: 2.5831737518310547
    Step: 5000, Train Loss: 2.5520806312561035, Val Loss: 2.587939500808716
    Step: 5100, Train Loss: 2.558637857437134, Val Loss: 2.574836492538452
    Step: 5200, Train Loss: 2.5474486351013184, Val Loss: 2.56428861618042
    Step: 5300, Train Loss: 2.546694278717041, Val Loss: 2.5716660022735596
    Step: 5400, Train Loss: 2.541635274887085, Val Loss: 2.5547852516174316
    Step: 5500, Train Loss: 2.533029556274414, Val Loss: 2.5598087310791016
    Step: 5600, Train Loss: 2.5308423042297363, Val Loss: 2.5512118339538574
    Step: 5700, Train Loss: 2.524716854095459, Val Loss: 2.5538253784179688
    Step: 5800, Train Loss: 2.532630443572998, Val Loss: 2.5539026260375977
    Step: 5900, Train Loss: 2.5297341346740723, Val Loss: 2.544002056121826
    Step: 6000, Train Loss: 2.5110831260681152, Val Loss: 2.536102056503296
    Step: 6100, Train Loss: 2.5155181884765625, Val Loss: 2.548837661743164
    Step: 6200, Train Loss: 2.507169008255005, Val Loss: 2.530263900756836
    Step: 6300, Train Loss: 2.5148274898529053, Val Loss: 2.533947706222534
    Step: 6400, Train Loss: 2.5012450218200684, Val Loss: 2.5191521644592285
    Step: 6500, Train Loss: 2.495518445968628, Val Loss: 2.5320611000061035
    Step: 6600, Train Loss: 2.512810707092285, Val Loss: 2.5210649967193604
    Step: 6700, Train Loss: 2.501819610595703, Val Loss: 2.519747734069824
    Step: 6800, Train Loss: 2.50238037109375, Val Loss: 2.515887498855591
    Step: 6900, Train Loss: 2.499113082885742, Val Loss: 2.5177834033966064
    Step: 7000, Train Loss: 2.4962542057037354, Val Loss: 2.5254430770874023
    Step: 7100, Train Loss: 2.5049450397491455, Val Loss: 2.517500162124634
    Step: 7200, Train Loss: 2.4947986602783203, Val Loss: 2.520648956298828
    Step: 7300, Train Loss: 2.487584114074707, Val Loss: 2.5127639770507812
    Step: 7400, Train Loss: 2.4888434410095215, Val Loss: 2.5234220027923584
    Step: 7500, Train Loss: 2.4871115684509277, Val Loss: 2.514145851135254
    Step: 7600, Train Loss: 2.4916443824768066, Val Loss: 2.5017917156219482
    Step: 7700, Train Loss: 2.4748964309692383, Val Loss: 2.503528118133545
    Step: 7800, Train Loss: 2.487521171569824, Val Loss: 2.5096848011016846
    Step: 7900, Train Loss: 2.488088607788086, Val Loss: 2.512087106704712
    Step: 8000, Train Loss: 2.4777252674102783, Val Loss: 2.503478527069092
    Step: 8100, Train Loss: 2.474973201751709, Val Loss: 2.5001137256622314
    Step: 8200, Train Loss: 2.476576328277588, Val Loss: 2.5093953609466553
    Step: 8300, Train Loss: 2.4765419960021973, Val Loss: 2.511681318283081
    Step: 8400, Train Loss: 2.474428176879883, Val Loss: 2.504129409790039
    Step: 8500, Train Loss: 2.480362892150879, Val Loss: 2.503594160079956
    Step: 8600, Train Loss: 2.4797637462615967, Val Loss: 2.497915267944336
    Step: 8700, Train Loss: 2.4743828773498535, Val Loss: 2.4981327056884766
    Step: 8800, Train Loss: 2.4769251346588135, Val Loss: 2.498363971710205
    Step: 8900, Train Loss: 2.4670166969299316, Val Loss: 2.4984006881713867
    Step: 9000, Train Loss: 2.4826412200927734, Val Loss: 2.492518663406372
    Step: 9100, Train Loss: 2.473421573638916, Val Loss: 2.4957001209259033
    Step: 9200, Train Loss: 2.4733846187591553, Val Loss: 2.5022668838500977
    Step: 9300, Train Loss: 2.468461036682129, Val Loss: 2.4921517372131348
    Step: 9400, Train Loss: 2.4772510528564453, Val Loss: 2.5064215660095215
    Step: 9500, Train Loss: 2.4709935188293457, Val Loss: 2.496542453765869
    Step: 9600, Train Loss: 2.468536853790283, Val Loss: 2.495712995529175
    Step: 9700, Train Loss: 2.4661900997161865, Val Loss: 2.4918875694274902
    Step: 9800, Train Loss: 2.466442346572876, Val Loss: 2.4893364906311035
    Step: 9900, Train Loss: 2.461491346359253, Val Loss: 2.5023200511932373
    Loss = 2.412075996398926


### 3.2. Generating Text From Trained B-Gram Model


```python
idx = torch.zeros((1,1), dtype=torch.long, device=device)
new_tokens = model.generate(idx, 500)[0].tolist()

# decode the generated tokens
gen_txt = decode(new_tokens)
print(gen_txt)
```

    
    Oryog ARKEve tou aroY:
    
    Wanthintis n Thimed;
    Hou f l ait ckin by mapa, owis.
    Andean she dva
    Agtut:
    
    An t br'rtrercad locheres.
    I oly I t onddocint biss be ceed vemmy I ngowhinothabld
    tat hel I ompatyor yo t.
    IIORIDe s, ir Bewilf sof ate k suser s y usprgr!
    TUKiler igerindsolle CESIOFormegine s f pr'my.
    US: ghiele.
    ORe man thethes smm mu hen ffang pend.
    
    CELENorfqve.
    Whivingr ny atTheerklour, ad, othendete t rea'
    LYo w? hor landinthes s h anofok, y thupe s celd
    ul ayoe imovitod ane'l,
    G st youo o


Now, looking at the result, we can see that the model has learned to generate text that looks like Shakespeare. However, the result is of low quality. This is because model is still quite simple and can be improved. To improve the model, we would like to learn the inter-relations between tokens in the sentence, even if they are far apart. This is where the **transformer** model comes in.

## 4. Building the Transformer Model
Up until now, using only the `Bigram Language Model`, we could not learn the inter relations between tokens. To predict the next token, we only used the previous token. **The transformer model, on the other hand, can learn the inter-relations between tokens, even if they are far apart.**

### 4.1 The Attention Module - Learning the Relations Between Tokens

In a language model, we would like to predict the next token, based only of preivous (context) tokens. This is different from models used for images, where we would like to learn the inter relations between one token, and the entire set of tokens representing the image.

The attention module, which is the at the core of the transformer model, is the building block used to learn and represent the inter-relations between tokens. When the attention module is applied to a sequence of tokens coming from the same input sequence of tokens, it is called **self-attention**. When the attention module is applied to two different sequences of tokens, it is called **cross-attention**. 


### 4.2 Preliminaries - Represting the Information of Previous Tokens as Average
As a first step, we would like to develop a method to represent the information of previous tokens. The most simple way to do that, would be to average the embeddings of all previous tokens.

Lets look a the next toy example:
Lets say our `batch` size is `4`, our `block_size` is `8`, and out `vocab_size` is 2.


```python
import torch
torch.manual_seed(1337)
B,T,C = 4,8,2 #batch size, time, channels

x = torch.randn(B,T,C)
x.shape
```




    torch.Size([4, 8, 2])




```python
# We want to create a new tensor, where each element in the T dimension,
# is the mean of all elements before it:
# x[b,t] = mean_{i<=t} x[b,i]

xbow = torch.zeros((B,T,C))
for b in range(B):
    for t in range(T):
        xprev = x[b, :t+1]
        xbow[b,t] = torch.mean(xprev, 0)

print(x[0])
print(xbow[0])
```

    tensor([[ 0.1808, -0.0700],
            [-0.3596, -0.9152],
            [ 0.6258,  0.0255],
            [ 0.9545,  0.0643],
            [ 0.3612,  1.1679],
            [-1.3499, -0.5102],
            [ 0.2360, -0.2398],
            [-0.9211,  1.5433]])
    tensor([[ 0.1808, -0.0700],
            [-0.0894, -0.4926],
            [ 0.1490, -0.3199],
            [ 0.3504, -0.2238],
            [ 0.3525,  0.0545],
            [ 0.0688, -0.0396],
            [ 0.0927, -0.0682],
            [-0.0341,  0.1332]])


#### 4.2.1 A Mathematical Trick

Although the above method outputs the desired averaging, because it uses `for` loops, it is very inefficient. To improve the efficency, we will use matrix multiplication.

A multipication of every matrix with a square matrix of size `nxn`, filled with `1, results in the summation of the rows.

So given the matrices:


```python
a = torch.ones(3,3)
b = torch.randint(0,10, (3,2)).float()

print('a=')
print(a)
print('b=')
print(b)
```

    a=
    tensor([[1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.]])
    b=
    tensor([[8., 6.],
            [5., 2.],
            [4., 4.]])


The running sum of the rows of `b` is given by:


```python
c = a@b
print('c=')
print(c)
```

    c=
    tensor([[17., 12.],
            [17., 12.],
            [17., 12.]])


Now, because matrix `a` if a full mask of `1`, the result of all the rows in `c` is the same (we sum all rows in `b`).
If instead, matrix `a` will be a lower triangular matrix:


```python
tril = torch.tril(torch.ones(3,3))
print(tril)
```

    tensor([[1., 0., 0.],
            [1., 1., 0.],
            [1., 1., 1.]])


Every element in `c` will be the sum of all previous elements in the same column in `b`:


```python
c = tril@b
print('c=')
print(c)
```

    c=
    tensor([[ 8.,  6.],
            [13.,  8.],
            [17., 12.]])


Finally, becuase we want to average all the preivous elements, we can normalize matrix `a`, so that the sum of each row is `1`


```python
tril = torch.tril(torch.ones(3,3))
a = tril / torch.sum(tril, dim=1, keepdim=True)
print('a=')
print(a)
```

    a=
    tensor([[1.0000, 0.0000, 0.0000],
            [0.5000, 0.5000, 0.0000],
            [0.3333, 0.3333, 0.3333]])


So now, multiplying `a` by `b` will give us the desired result:


```python
print('b=')
print(b)

c = a@b
print('c=')
print(c)
```

    b=
    tensor([[8., 6.],
            [5., 2.],
            [4., 4.]])
    c=
    tensor([[8.0000, 6.0000],
            [6.5000, 4.0000],
            [5.6667, 4.0000]])


Returning to the original `xbow`, whith size `(B,T,C)`, we would like to do the same trick, with batch multiplcation. 
So, if we have a tensor of size `(T,T)` and we multiply it by a tensor of size `(B,T,C)`, pytorch will autmalically broadcast the first tensor to size `(B,T,T)`, and multiply it by the second tensor.

`(B,T,T) x (B,T,C) = (B,T,C)`


```python
wei = torch.tril(torch.ones(T, T))
wei = wei / wei.sum(1, keepdim=True)
xbow2 = wei@x

print ("xbow and xbow are the same. For example")
print("xbow[0,:3] ")
print(xbow[0,:3])
print("xbow2[0,:3] ")
print(xbow2[0,:3])

```

    xbow and xbow are the same. For example
    xbow[0,:3] 
    tensor([[ 0.1808, -0.0700],
            [-0.0894, -0.4926],
            [ 0.1490, -0.3199]])
    xbow2[0,:3] 
    tensor([[ 0.1808, -0.0700],
            [-0.0894, -0.4926],
            [ 0.1490, -0.3199]])


#### 4.2.2. Using Softmax
When we train self attension module, we would like the weights to be learned. Thats because not every token will be equally important. So instead of using averaging, we are going to use `softmax` normalization.

We start by initializing all of our weights to `0`.


```python
wei2 = torch.zeros(T, T)
wei2
```




    tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0.]])



Because we want to aggerate only past information, we will set all future elements to `-inf`. This will make the softmax of all future elements to `0`.


```python
tril = torch.tril(torch.ones(T, T))
wei2 = wei2.masked_fill(tril==0, float('-inf'))
wei2
```




    tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
            [0., 0., 0., -inf, -inf, -inf, -inf, -inf],
            [0., 0., 0., 0., -inf, -inf, -inf, -inf],
            [0., 0., 0., 0., 0., -inf, -inf, -inf],
            [0., 0., 0., 0., 0., 0., -inf, -inf],
            [0., 0., 0., 0., 0., 0., 0., -inf],
            [0., 0., 0., 0., 0., 0., 0., 0.]])



Now we are going to apply softmax function, and we will get the desired result.


```python
wei2 = F.softmax(wei2, dim=1)
wei2
```




    tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],
            [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],
            [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],
            [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])



And the new `xbow` will be:


```python
xbow3 = wei2@x
print("Are xbow2 and xbow3 the same: ", torch.allclose(xbow2, xbow3))
```

    Are xbow2 and xbow3 the same:  True


### 5. Building the Self-Attention Block
In the above section, represented the information stored in previous tokens as an average of their embeddings. As seen from `xbow` matrices, this causes the embeddings to have similar weight. 
Instead, we need each of the embedding to have a different weight based on the current embedding. For example, if the current embedding is a noun, we would like the weights of the other embeddings to be higher if they are verbs, and lower if they are adjectives.

To do this, we propose two more vectors. The `query` vector `Q`, and the `key` vector `K`. 
Given the current token, the `Q` vector represents `what am I looking for`. The `K` vector represents `what do I have`. 
Analogous to the noun example, if the current token is a noun, the `Q` vector will have high weights where it expects verbs to be, and low weights where it expects adjectives to be. The `K` vector will have high weights where verbs are, and low weights where adjectives are.

`Q` and `K` are learnable vectors. 


```python
head_size = 16
key = nn.Linear(C, head_size, bias=False)
query = nn.Linear(C, head_size, bias=False)

x = torch.randn(B, T, C)

k = key(x)  #(B,T,16)
q = query(x) #(B,T,16)

wei = q@k.transpose(-2, -1) # (B,T,16) @ (B,16,T) --> (B, T, T)

wei[0]
```




    tensor([[ 0.5092,  0.8633, -0.0858, -0.2769,  0.2295, -0.2686, -0.4146, -0.7440],
            [-0.2630,  1.1151,  0.3556, -0.2197,  0.8090, -0.3172, -0.3451,  0.3032],
            [-0.3104, -0.2149,  0.1144,  0.0964,  0.0451,  0.0728,  0.1412,  0.4374],
            [-0.0152, -0.3885, -0.0698,  0.0925, -0.2224,  0.1140,  0.1423,  0.0410],
            [-0.4398,  0.1820,  0.2591,  0.0236,  0.3530, -0.0390,  0.0257,  0.5944],
            [ 0.0604, -0.3536, -0.1011,  0.0731, -0.2437,  0.1013,  0.1142, -0.0645],
            [-0.0111, -0.5781, -0.1097,  0.1360, -0.3373,  0.1692,  0.2094,  0.0453],
            [-0.6855, -1.2433,  0.0993,  0.3916, -0.3571,  0.3853,  0.5872,  1.0059]],
           grad_fn=<SelectBackward0>)



Now, because we don't want to aggeragate any information from future tokens, we are going to use the same trick as before - use a lower triangular matrix and set all future elements to `-inf`.


```python
tril = torch.tril(torch.ones(T, T))
wei = wei.masked_fill(tril==0, float('-inf'))
wei = F.softmax(wei, dim=-1)
wei[0]
```




    tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.2013, 0.7987, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.2755, 0.3031, 0.4213, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.2667, 0.1836, 0.2526, 0.2971, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.1153, 0.2147, 0.2319, 0.1833, 0.2548, 0.0000, 0.0000, 0.0000],
            [0.1885, 0.1246, 0.1604, 0.1909, 0.1391, 0.1964, 0.0000, 0.0000],
            [0.1471, 0.0834, 0.1333, 0.1704, 0.1062, 0.1762, 0.1834, 0.0000],
            [0.0500, 0.0286, 0.1096, 0.1468, 0.0694, 0.1459, 0.1785, 0.2713]],
           grad_fn=<SelectBackward0>)



The last thing we need to do is to multiply the weights with the value `V` of the current token - also a learnable vector - and normalize with the size of the embedding.
The normalization is important to keep the variance of a normal distribution to be `1`, so that the weights don't become one hot vectors.


```python
value = nn.Linear(C, head_size, bias=False)
v = value(x) #(B,T,16)

tril = torch.tril(torch.ones(T, T))
wei = head_size**(-0.5)*wei.masked_fill(tril==0, float('-inf'))
wei = F.softmax(wei, dim=-1)

out = wei@v
v[0]
```




    tensor([[ 0.8914, -0.4325,  1.1267, -0.4434,  1.1065,  0.1209,  0.5062,  0.6908,
              0.4730,  0.3906, -0.4074,  0.9151, -1.0772, -0.4376, -0.7330,  0.5257],
            [-0.5965, -0.8033, -0.6086,  0.5651,  0.1012, -0.5463, -0.6486, -0.2655,
              0.1886, -0.5214,  0.1652,  0.6625,  0.6457, -0.1876, -0.5637, -0.1674],
            [-0.5705,  0.0589, -0.6921,  0.3373, -0.5403, -0.1702, -0.3857, -0.4029,
             -0.2020, -0.3019,  0.2393, -0.3314,  0.6744,  0.1842,  0.2589, -0.2997],
            [ 0.0050,  0.2514, -0.0274, -0.0649, -0.1893,  0.1088,  0.0749, -0.0418,
             -0.1147,  0.0626,  0.0227, -0.2910,  0.0114,  0.1092,  0.2408, -0.0399],
            [-0.8507, -0.2366, -0.9889,  0.5827, -0.5559, -0.3919, -0.6672, -0.5424,
             -0.1512, -0.5273,  0.3250, -0.1158,  0.9834,  0.1321,  0.0731, -0.3922],
            [ 0.1455,  0.2486,  0.1414, -0.1508, -0.0653,  0.1557,  0.1731,  0.0553,
             -0.0704,  0.1397, -0.0351, -0.2231, -0.1538,  0.0689,  0.1883,  0.0319],
            [ 0.0293,  0.3772, -0.0150, -0.1108, -0.2651,  0.1707,  0.1277, -0.0478,
             -0.1654,  0.1060,  0.0251, -0.4266, -0.0085,  0.1577,  0.3536, -0.0488],
            [-1.1930,  0.6356, -1.5154,  0.5795, -1.5246, -0.1376, -0.6613, -0.9347,
             -0.6593, -0.5093,  0.5508, -1.2909,  1.4455,  0.6106,  1.0357, -0.7132]],
           grad_fn=<SelectBackward0>)



#### **So, finally here is the attention module:**


```python
class SelfAttentionHead(nn.Module):
    """One head of self attention"""

    def __init__(self, n_embd, head_size, block_size):
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
    
    def forward(self, x):
        B,T,C = x.shape
        k = self.key(x)
        q = self.query(x)
        v = self.value(x)
        
        wei= q @ k.transpose(-2, -1) * C**(-0.5)
        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))
        wei = F.softmax(wei, dim=-1)
        
        out = wei @ v
        return out
```

### 6. Integrating the Self-Attention Module into the Bigram Language Model
Now, lets add the self-attention module to the bigram model, so that it will learn the inter-relations between tokens. 
To do this, we first reduce the number of embeddings in the token embedding layer, and instead add a linear layer after the embedding layer. This will allow the model to learn more complex patterns in the data and increase stability.

Second, we add a positional encoding to the input embeddings. This is important because it helps the model learn the relations between tokens also based on their position in the sequence.


```python
class BigramLanguageModelSignleAttention(nn.Module):
    """ Bigram Language Model.
        Args:
            vocab_size (int): Number of tokens (size of vocabulary)
    """
 
    def __init__(self, vocab_size, n_embd=24):
        super().__init__()
        # Each token directiry reads off the logits for the next token from a lookup table
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.pos_embedding_table = nn.Embedding(block_size, n_embd)
        self.atten_head = SelfAttentionHead(n_embd, head_size=16, block_size=block_size)
        self.head = nn.Linear(head_size, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        token_embd = self.token_embedding_table(idx) # (B,T,n_embd)
        pos_embd = self.pos_embedding_table(torch.arange(T).to(device)) # (T,n_embd)
        x = token_embd + pos_embd   # adding positional embedding to token embedding
        x = self.atten_head(x) # apply one head of attention
        logits = self.head(x) # (B,T,vocab_size)
        # cross_entropy expects the input (logits) to be of shape (B, C, T)
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            loss = F.cross_entropy(logits.view(B*T, C),  targets.view(B*T))

        return logits ,loss

    def generate(self, idx, max_new_tokens):
        """ Generate new tokens, given current context.
            In a simple B-gram model, we just need to look up the logits for the next token.
            In more complex models, we are also intrested in the history of the tokens. In other
            words, a larger context.

            Args:
                idx (Tensor): Initial context
                max_new_tokens (int): Maximum number of new tokens to generate.
            Returns:
                Tensor: Generated tokens

        """

        # idx is a tensor of shape (B, T)
        idx.to(device)
        for i in range(max_new_tokens):
            idx_cond = idx[:, -block_size:] # only consider the last block_size tokens
            # get the prediction for the current context
            logits, loss = self.forward(idx_cond) # logits shape B, T, C
            # In B-gram model, focus only on the last time step.
            logits = logits[:, -1, :]   
            # convert logits to probabilities
            probs = F.softmax(logits, dim=-1)
            # sample the next token from the distribution
            next_token = torch.multinomial(probs, num_samples=1).to(device)
            idx = torch.cat([idx, next_token], dim=1) # shape B, T+1

        return idx
            
            


model = BigramLanguageModelSignleAttention(vocab_size)
model.to(device)
out, loss = model(xb, yb)
print(out.shape)
print(loss)

# We expect the loss to be around -log(1/vocab_size) = -log(1/65) = 4.17
print(f"Expected loss for uniform distribution: {-torch.log(torch.tensor(1/vocab_size))}")

# Example - generate new tokens
print("\n")
print("Example - generate new tokens")
print("(Model is not trained yet, so it will generate random tokens)")
idx = torch.zeros((1,1), dtype=torch.long).to(device)
new_tokens = model.generate(idx, 500)[0].tolist()

# decode the generated tokens
gen_txt = decode(new_tokens)
print(gen_txt)
```

    torch.Size([32, 8, 65])
    tensor(4.2162, grad_fn=<NllLossBackward0>)
    Expected loss for uniform distribution: 4.174387454986572
    
    
    Example - generate new tokens
    (Model is not trained yet, so it will generate random tokens)
    
    G3 yKKaJ:OFP3 myihe;mKE?XsGSKW!hbjaCSQ!'DZHn:xDHSuNYfP
    kZHBbNk$?MnXLoj,pFcn3AHu,u!G'v-W::3cDKHkfb;.VfZwhLnbuNcW$3Jz,&;Px?TdBF:PYCQ&-Lq'ZvtJ&G:!;EnRXVNpQZ$QAtyfbMfslt,yL;DDH ED3g'In:hTh;$PnPrUct:Squp-.IDX
    ?3,.HNXwce:3H3naRxRKWT$fXNMMepd.-Whi E:E?:umFaLxgzZ:QBWDXTwO,iy;
    vfECZlw-BtVVwbfLImnghhSb?hRED
    YOwc3LsPETowotLjeIP'Idh!ZxuZRS 
    fc&?ohED.3$lpoO!-vwmvaFcfhiWNPbGro?
    qLIMRQ'fZpqttSbd$3RRQhJCs;nRhlq!gLfKMy,RWJaecP!dyH?cDcg:exX
    xa!gHEQ
    -JDfeKiXylxltjxKYYxB-bBG-EYEqda$,
    k3dwa;DjtIld!sPekz
    yATdcMn?b'th


#### Training the Model

Lets see if the model has improved.


```python
# define the training loop
num_epochs = 10000
batch_size = 32
eval_interval = 100
lr = 1e-3

model = BigramLanguageModelSignleAttention(vocab_size)
model = model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
for step in range(num_epochs):
    
    if step % eval_interval == 0:
        losses = estimate_loss()
        print(f"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}")

    # sample a batch of data
    xb, yb = get_batch(split="train")
    # forward pass
    logits, loss = model(xb, yb)

    # backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

losses = estimate_loss()
print(f"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}")
```

    Step: 0, Train Loss: 4.318869113922119, Val Loss: 4.316243648529053
    Step: 100, Train Loss: 3.361433744430542, Val Loss: 3.4011871814727783
    Step: 200, Train Loss: 3.1943845748901367, Val Loss: 3.22554087638855
    Step: 300, Train Loss: 3.101339340209961, Val Loss: 3.1098291873931885
    Step: 400, Train Loss: 3.005418300628662, Val Loss: 2.986083984375
    Step: 500, Train Loss: 2.899444580078125, Val Loss: 2.897247076034546
    Step: 600, Train Loss: 2.8042774200439453, Val Loss: 2.8046178817749023
    Step: 700, Train Loss: 2.751274824142456, Val Loss: 2.746579170227051
    Step: 800, Train Loss: 2.711836576461792, Val Loss: 2.707810878753662
    Step: 900, Train Loss: 2.6862480640411377, Val Loss: 2.678891658782959
    Step: 1000, Train Loss: 2.6427645683288574, Val Loss: 2.6360716819763184
    Step: 1100, Train Loss: 2.6075620651245117, Val Loss: 2.610530138015747
    Step: 1200, Train Loss: 2.5975911617279053, Val Loss: 2.584049701690674
    Step: 1300, Train Loss: 2.5819640159606934, Val Loss: 2.5862700939178467
    Step: 1400, Train Loss: 2.5630404949188232, Val Loss: 2.5606067180633545
    Step: 1500, Train Loss: 2.5509963035583496, Val Loss: 2.546405553817749
    Step: 1600, Train Loss: 2.547877788543701, Val Loss: 2.545130968093872
    Step: 1700, Train Loss: 2.5391831398010254, Val Loss: 2.5315020084381104
    Step: 1800, Train Loss: 2.5320966243743896, Val Loss: 2.531750440597534
    Step: 1900, Train Loss: 2.511298418045044, Val Loss: 2.5184926986694336
    Step: 2000, Train Loss: 2.5279479026794434, Val Loss: 2.519688129425049
    Step: 2100, Train Loss: 2.5107696056365967, Val Loss: 2.5037436485290527
    Step: 2200, Train Loss: 2.505211353302002, Val Loss: 2.4954099655151367
    Step: 2300, Train Loss: 2.4983136653900146, Val Loss: 2.4970345497131348
    Step: 2400, Train Loss: 2.499886989593506, Val Loss: 2.485172986984253
    Step: 2500, Train Loss: 2.4835970401763916, Val Loss: 2.4849300384521484
    Step: 2600, Train Loss: 2.4776461124420166, Val Loss: 2.4905364513397217
    Step: 2700, Train Loss: 2.4837517738342285, Val Loss: 2.475339651107788
    Step: 2800, Train Loss: 2.481426954269409, Val Loss: 2.4716575145721436
    Step: 2900, Train Loss: 2.471475601196289, Val Loss: 2.4696128368377686
    Step: 3000, Train Loss: 2.4732825756073, Val Loss: 2.4637389183044434
    Step: 3100, Train Loss: 2.4717066287994385, Val Loss: 2.4659347534179688
    Step: 3200, Train Loss: 2.4527573585510254, Val Loss: 2.461484909057617
    Step: 3300, Train Loss: 2.462925672531128, Val Loss: 2.4657058715820312
    Step: 3400, Train Loss: 2.4607133865356445, Val Loss: 2.462747097015381
    Step: 3500, Train Loss: 2.4513535499572754, Val Loss: 2.4653754234313965
    Step: 3600, Train Loss: 2.444948434829712, Val Loss: 2.454998254776001
    Step: 3700, Train Loss: 2.448162317276001, Val Loss: 2.4530279636383057
    Step: 3800, Train Loss: 2.4607582092285156, Val Loss: 2.457272529602051
    Step: 3900, Train Loss: 2.4421045780181885, Val Loss: 2.4555106163024902
    Step: 4000, Train Loss: 2.4468860626220703, Val Loss: 2.452601432800293
    Step: 4100, Train Loss: 2.445605754852295, Val Loss: 2.453697681427002
    Step: 4200, Train Loss: 2.4442498683929443, Val Loss: 2.450683832168579
    Step: 4300, Train Loss: 2.437619924545288, Val Loss: 2.4515228271484375
    Step: 4400, Train Loss: 2.4358184337615967, Val Loss: 2.449371576309204
    Step: 4500, Train Loss: 2.434701919555664, Val Loss: 2.438138484954834
    Step: 4600, Train Loss: 2.439465284347534, Val Loss: 2.4472765922546387
    Step: 4700, Train Loss: 2.4381253719329834, Val Loss: 2.4446909427642822
    Step: 4800, Train Loss: 2.444300889968872, Val Loss: 2.4475204944610596
    Step: 4900, Train Loss: 2.4390945434570312, Val Loss: 2.4390225410461426
    Step: 5000, Train Loss: 2.4330568313598633, Val Loss: 2.4427685737609863
    Step: 5100, Train Loss: 2.4316108226776123, Val Loss: 2.4448728561401367
    Step: 5200, Train Loss: 2.420999526977539, Val Loss: 2.436650276184082
    Step: 5300, Train Loss: 2.425389051437378, Val Loss: 2.4403204917907715
    Step: 5400, Train Loss: 2.423203468322754, Val Loss: 2.4328510761260986
    Step: 5500, Train Loss: 2.4236888885498047, Val Loss: 2.4420931339263916
    Step: 5600, Train Loss: 2.4187488555908203, Val Loss: 2.438521385192871
    Step: 5700, Train Loss: 2.4205994606018066, Val Loss: 2.436563014984131
    Step: 5800, Train Loss: 2.4137372970581055, Val Loss: 2.441340923309326
    Step: 5900, Train Loss: 2.4191112518310547, Val Loss: 2.432573080062866
    Step: 6000, Train Loss: 2.4139394760131836, Val Loss: 2.4225432872772217
    Step: 6100, Train Loss: 2.4109573364257812, Val Loss: 2.424131393432617
    Step: 6200, Train Loss: 2.4080283641815186, Val Loss: 2.4285576343536377
    Step: 6300, Train Loss: 2.4059743881225586, Val Loss: 2.428717613220215
    Step: 6400, Train Loss: 2.4048752784729004, Val Loss: 2.412459135055542
    Step: 6500, Train Loss: 2.414376735687256, Val Loss: 2.4179840087890625
    Step: 6600, Train Loss: 2.4189679622650146, Val Loss: 2.412946939468384
    Step: 6700, Train Loss: 2.411099672317505, Val Loss: 2.4203507900238037
    Step: 6800, Train Loss: 2.414649486541748, Val Loss: 2.4224777221679688
    Step: 6900, Train Loss: 2.406303882598877, Val Loss: 2.4133710861206055
    Step: 7000, Train Loss: 2.4137215614318848, Val Loss: 2.422297954559326
    Step: 7100, Train Loss: 2.4112820625305176, Val Loss: 2.422137975692749
    Step: 7200, Train Loss: 2.419929027557373, Val Loss: 2.4298295974731445
    Step: 7300, Train Loss: 2.412686824798584, Val Loss: 2.4136548042297363
    Step: 7400, Train Loss: 2.411269426345825, Val Loss: 2.4241864681243896
    Step: 7500, Train Loss: 2.4120094776153564, Val Loss: 2.4249303340911865
    Step: 7600, Train Loss: 2.407221794128418, Val Loss: 2.417696952819824
    Step: 7700, Train Loss: 2.403322219848633, Val Loss: 2.405897378921509
    Step: 7800, Train Loss: 2.4019994735717773, Val Loss: 2.4162492752075195
    Step: 7900, Train Loss: 2.4110822677612305, Val Loss: 2.413710832595825
    Step: 8000, Train Loss: 2.392925977706909, Val Loss: 2.421046018600464
    Step: 8100, Train Loss: 2.398772954940796, Val Loss: 2.4163715839385986
    Step: 8200, Train Loss: 2.400676965713501, Val Loss: 2.4106833934783936
    Step: 8300, Train Loss: 2.4011054039001465, Val Loss: 2.4206228256225586
    Step: 8400, Train Loss: 2.3984310626983643, Val Loss: 2.4195737838745117
    Step: 8500, Train Loss: 2.4017579555511475, Val Loss: 2.419262170791626
    Step: 8600, Train Loss: 2.397552013397217, Val Loss: 2.4141533374786377
    Step: 8700, Train Loss: 2.3888285160064697, Val Loss: 2.4114654064178467
    Step: 8800, Train Loss: 2.387735366821289, Val Loss: 2.4129397869110107
    Step: 8900, Train Loss: 2.3874781131744385, Val Loss: 2.3971829414367676
    Step: 9000, Train Loss: 2.4044830799102783, Val Loss: 2.420161247253418
    Step: 9100, Train Loss: 2.400146722793579, Val Loss: 2.4141616821289062
    Step: 9200, Train Loss: 2.398380994796753, Val Loss: 2.4100191593170166
    Step: 9300, Train Loss: 2.3882038593292236, Val Loss: 2.4072606563568115
    Step: 9400, Train Loss: 2.3911397457122803, Val Loss: 2.417815923690796
    Step: 9500, Train Loss: 2.399789333343506, Val Loss: 2.4174346923828125
    Step: 9600, Train Loss: 2.3975493907928467, Val Loss: 2.4143831729888916
    Step: 9700, Train Loss: 2.394803047180176, Val Loss: 2.40925931930542
    Step: 9800, Train Loss: 2.3937323093414307, Val Loss: 2.405217409133911
    Step: 9900, Train Loss: 2.3991994857788086, Val Loss: 2.40938138961792
    Loss = 2.1585164070129395


Previously we got the following loss:

`Step: 9900, Train Loss: 2.461491346359253, Val Loss: 2.5023200511932373`

After training the model with the self attention block, we get the following loss:

`Step: 9900, Train Loss: 2.3991994857788086, Val Loss: 2.40938138961792`

So we got some improvement.

### 7. Further Improvement: Multi-Head Attention

A single attention head can only learn a single type of inter relations between tokens. To learn multiple types of inter-relations, we can use multiple attention heads in parallel. This is called `multi-head attention`.


![multi-head-attention](/assets/images/gpt/attention_heads.png)

To create a multi-head attention model, all we have to do is to create multiple `Q`, `K`, and `V` vectors, and apply the self-attention module to each of them. Each of the single attention heads will be concatenated, and passed through a linear layer to reduce the dimensionality back to the original size. 


```python
class MultiHeadAttention(nn.Module):
    """Multi-head self attention"""

    def __init__(self, n_heads, n_embd, head_size, block_size):
        super().__init__()        
        self.heads = nn.ModuleList([SelfAttentionHead(n_embd, head_size, block_size) for _ in range(n_heads)])
        
    def forward(self, x):
        out = torch.cat([head(x) for head in self.heads], dim=-1)
        return out
```

Now lets integrate the multi-head attention into our model.
The important thing to note is reduce the number of embeddings according to the number of heads, so once they are concatenated they will have the same size as the original embeddings.


```python
class BigramLanguageModelMultiHeadAttention(nn.Module):
    """ Bigram Language Model.
        Args:
            vocab_size (int): Number of tokens (size of vocabulary)
    """
 
    def __init__(self, vocab_size, n_embd=32, n_heads=4):
        super().__init__()
        # Each token directiry reads off the logits for the next token from a lookup table
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.pos_embedding_table = nn.Embedding(block_size, n_embd)
        head_size = n_embd // n_heads
        self.multi_att_head = MultiHeadAttention(n_heads=n_heads, n_embd=n_embd, head_size=head_size, block_size=block_size)
        self.head = nn.Linear(n_embd, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        token_embd = self.token_embedding_table(idx) # (B,T,n_embd)
        pos_embd = self.pos_embedding_table(torch.arange(T).to(device)) # (T,n_embd)
        x = token_embd + pos_embd   # adding positional embedding to token embedding
        x = self.multi_att_head(x) # apply one head of attention
        logits = self.head(x) # (B,T,vocab_size)
        # cross_entropy expects the input (logits) to be of shape (B, C, T)
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            loss = F.cross_entropy(logits.view(B*T, C),  targets.view(B*T))

        return logits ,loss

    def generate(self, idx, max_new_tokens):
        """ Generate new tokens, given current context.
            In a simple B-gram model, we just need to look up the logits for the next token.
            In more complex models, we are also intrested in the history of the tokens. In other
            words, a larger context.

            Args:
                idx (Tensor): Initial context
                max_new_tokens (int): Maximum number of new tokens to generate.
            Returns:
                Tensor: Generated tokens

        """

        # idx is a tensor of shape (B, T)
        idx.to(device)
        for i in range(max_new_tokens):
            idx_cond = idx[:, -block_size:] # only consider the last block_size tokens
            # get the prediction for the current context
            logits, loss = self.forward(idx_cond) # logits shape B, T, C
            # In B-gram model, focus only on the last time step.
            logits = logits[:, -1, :]   
            # convert logits to probabilities
            probs = F.softmax(logits, dim=-1)
            # sample the next token from the distribution
            next_token = torch.multinomial(probs, num_samples=1).to(device)
            idx = torch.cat([idx, next_token], dim=1) # shape B, T+1

        return idx
            
            


model = BigramLanguageModelMultiHeadAttention(vocab_size)
model.to(device)
out, loss = model(xb, yb)
print(out.shape)
print(loss)

# We expect the loss to be around -log(1/vocab_size) = -log(1/65) = 4.17
print(f"Expected loss for uniform distribution: {-torch.log(torch.tensor(1/vocab_size))}")

# Example - generate new tokens
print("\n")
print("Example - generate new tokens")
print("(Model is not trained yet, so it will generate random tokens)")
idx = torch.zeros((1,1), dtype=torch.long).to(device)
new_tokens = model.generate(idx, 500)[0].tolist()

# decode the generated tokens
gen_txt = decode(new_tokens)
print(gen_txt)
```

    torch.Size([32, 8, 65])
    tensor(4.2748, grad_fn=<NllLossBackward0>)
    Expected loss for uniform distribution: 4.174387454986572
    
    
    Example - generate new tokens
    (Model is not trained yet, so it will generate random tokens)
    
    WQ.&B.SgDDG.pi?K3a-FIDcm,YJRpBprSNTgQzHbUiunYj$lUYwIBc.RtPR'RLG$bVXmqo!MtTriMwxps;?-FJWmF''Y;S!sdVBYlnpgJtJD3?dP$FYMWKeNDYX?&;-$tVD;&b'.WAXXThWLgdnbFNMeY$t BaHmCY?lA,UemzsQ.PD? KLaQ,
    jC;DqXuqWIFgnet.OUJEViKh'F,!nM?xKTwaPwZDyqOQtHmzmYvkVyl:eom?-unESecKXUYVFTUINTyCe.qScM:?Rp'FlylepQ'SOb'rBgfd&UNF,NI3FHjGhCU,llrwo EX!mtNVHdOIT'D;IJinlkxBYIUjRb:EeClrBMmBBFl:xvWlzmOBY;:ewXcAsQwoHhonKWmhVXonytKf3vLS.cYVysJV?O
    CSnKeiaLBTgGdoWgNmrAHvTcU,3iSW.$;WLz,!AZeMLR
    ;IN'hVkG$SeTn$Q,uCtKKhXk3QmlcToOvUmy?TaP&UL$j&-S


Now lets train the new model and see if we have any improvement.


```python
# define the training loop
num_epochs = 10000
batch_size = 32
eval_interval = 100
lr = 1e-3

model = BigramLanguageModelMultiHeadAttention(vocab_size)
model = model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
for step in range(num_epochs):
    
    if step % eval_interval == 0:
        losses = estimate_loss()
        print(f"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}")

    # sample a batch of data
    xb, yb = get_batch(split="train")
    # forward pass
    logits, loss = model(xb, yb)

    # backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

losses = estimate_loss()
print(f"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}")
```

    Step: 0, Train Loss: 4.280439376831055, Val Loss: 4.277841091156006
    Step: 100, Train Loss: 3.1830806732177734, Val Loss: 3.2208504676818848
    Step: 200, Train Loss: 2.981065034866333, Val Loss: 2.9953842163085938
    Step: 300, Train Loss: 2.835463523864746, Val Loss: 2.8346798419952393
    Step: 400, Train Loss: 2.721287488937378, Val Loss: 2.7226738929748535
    Step: 500, Train Loss: 2.6442534923553467, Val Loss: 2.648371696472168
    Step: 600, Train Loss: 2.59602427482605, Val Loss: 2.592071771621704
    Step: 700, Train Loss: 2.5589630603790283, Val Loss: 2.5626206398010254
    Step: 800, Train Loss: 2.539506196975708, Val Loss: 2.5365712642669678
    Step: 900, Train Loss: 2.5071821212768555, Val Loss: 2.4975595474243164
    Step: 1000, Train Loss: 2.49436616897583, Val Loss: 2.4855751991271973
    Step: 1100, Train Loss: 2.471787452697754, Val Loss: 2.467949390411377
    Step: 1200, Train Loss: 2.468949556350708, Val Loss: 2.4682157039642334
    Step: 1300, Train Loss: 2.4356682300567627, Val Loss: 2.432159185409546
    Step: 1400, Train Loss: 2.4320151805877686, Val Loss: 2.424816608428955
    Step: 1500, Train Loss: 2.420280694961548, Val Loss: 2.421002149581909
    Step: 1600, Train Loss: 2.4068307876586914, Val Loss: 2.4121735095977783
    Step: 1700, Train Loss: 2.401731491088867, Val Loss: 2.394601821899414
    Step: 1800, Train Loss: 2.381617546081543, Val Loss: 2.392925977706909
    Step: 1900, Train Loss: 2.3756895065307617, Val Loss: 2.3858962059020996
    Step: 2000, Train Loss: 2.3775768280029297, Val Loss: 2.380723476409912
    Step: 2100, Train Loss: 2.365177631378174, Val Loss: 2.3653762340545654
    Step: 2200, Train Loss: 2.354142427444458, Val Loss: 2.362443208694458
    Step: 2300, Train Loss: 2.344810724258423, Val Loss: 2.3598475456237793
    Step: 2400, Train Loss: 2.351449966430664, Val Loss: 2.357290744781494
    Step: 2500, Train Loss: 2.335130214691162, Val Loss: 2.348008155822754
    Step: 2600, Train Loss: 2.33627986907959, Val Loss: 2.3524169921875
    Step: 2700, Train Loss: 2.3419294357299805, Val Loss: 2.3474924564361572
    Step: 2800, Train Loss: 2.327463388442993, Val Loss: 2.325650691986084
    Step: 2900, Train Loss: 2.3201491832733154, Val Loss: 2.331719160079956
    Step: 3000, Train Loss: 2.3141884803771973, Val Loss: 2.332568645477295
    Step: 3100, Train Loss: 2.3102962970733643, Val Loss: 2.3293421268463135
    Step: 3200, Train Loss: 2.306004524230957, Val Loss: 2.3110098838806152
    Step: 3300, Train Loss: 2.302025079727173, Val Loss: 2.322469711303711
    Step: 3400, Train Loss: 2.303490161895752, Val Loss: 2.3154494762420654
    Step: 3500, Train Loss: 2.3016388416290283, Val Loss: 2.3111634254455566
    Step: 3600, Train Loss: 2.292541742324829, Val Loss: 2.3008952140808105
    Step: 3700, Train Loss: 2.2889602184295654, Val Loss: 2.2995028495788574
    Step: 3800, Train Loss: 2.2901535034179688, Val Loss: 2.3069024085998535
    Step: 3900, Train Loss: 2.278183937072754, Val Loss: 2.3031551837921143
    Step: 4000, Train Loss: 2.2807013988494873, Val Loss: 2.2957499027252197
    Step: 4100, Train Loss: 2.2883710861206055, Val Loss: 2.2960317134857178
    Step: 4200, Train Loss: 2.274587631225586, Val Loss: 2.2865471839904785
    Step: 4300, Train Loss: 2.2689197063446045, Val Loss: 2.28869366645813
    Step: 4400, Train Loss: 2.261878252029419, Val Loss: 2.2937257289886475
    Step: 4500, Train Loss: 2.2647757530212402, Val Loss: 2.2949559688568115
    Step: 4600, Train Loss: 2.268130302429199, Val Loss: 2.291808843612671
    Step: 4700, Train Loss: 2.255236864089966, Val Loss: 2.279003143310547
    Step: 4800, Train Loss: 2.2535104751586914, Val Loss: 2.293456792831421
    Step: 4900, Train Loss: 2.2577576637268066, Val Loss: 2.278144121170044
    Step: 5000, Train Loss: 2.2594263553619385, Val Loss: 2.2768194675445557
    Step: 5100, Train Loss: 2.254304885864258, Val Loss: 2.2652416229248047
    Step: 5200, Train Loss: 2.241475820541382, Val Loss: 2.288149833679199
    Step: 5300, Train Loss: 2.255418539047241, Val Loss: 2.272458791732788
    Step: 5400, Train Loss: 2.236665964126587, Val Loss: 2.2566750049591064
    Step: 5500, Train Loss: 2.2365119457244873, Val Loss: 2.259363889694214
    Step: 5600, Train Loss: 2.2381441593170166, Val Loss: 2.2798023223876953
    Step: 5700, Train Loss: 2.2481305599212646, Val Loss: 2.2669644355773926
    Step: 5800, Train Loss: 2.232283592224121, Val Loss: 2.2597336769104004
    Step: 5900, Train Loss: 2.2312939167022705, Val Loss: 2.265692949295044
    Step: 6000, Train Loss: 2.2455642223358154, Val Loss: 2.262112617492676
    Step: 6100, Train Loss: 2.237785577774048, Val Loss: 2.2741587162017822
    Step: 6200, Train Loss: 2.2352492809295654, Val Loss: 2.2660324573516846
    Step: 6300, Train Loss: 2.229567766189575, Val Loss: 2.270669937133789
    Step: 6400, Train Loss: 2.2273340225219727, Val Loss: 2.245643138885498
    Step: 6500, Train Loss: 2.2176544666290283, Val Loss: 2.2700862884521484
    Step: 6600, Train Loss: 2.218446731567383, Val Loss: 2.255094051361084
    Step: 6700, Train Loss: 2.226853370666504, Val Loss: 2.2591171264648438
    Step: 6800, Train Loss: 2.2271246910095215, Val Loss: 2.2543489933013916
    Step: 6900, Train Loss: 2.217482566833496, Val Loss: 2.2414567470550537
    Step: 7000, Train Loss: 2.210606575012207, Val Loss: 2.2658350467681885
    Step: 7100, Train Loss: 2.2090418338775635, Val Loss: 2.2592933177948
    Step: 7200, Train Loss: 2.224485397338867, Val Loss: 2.245936393737793
    Step: 7300, Train Loss: 2.2058897018432617, Val Loss: 2.2471261024475098
    Step: 7400, Train Loss: 2.2152836322784424, Val Loss: 2.2577521800994873
    Step: 7500, Train Loss: 2.205692768096924, Val Loss: 2.2523999214172363
    Step: 7600, Train Loss: 2.205458879470825, Val Loss: 2.2556824684143066
    Step: 7700, Train Loss: 2.2078235149383545, Val Loss: 2.2545807361602783
    Step: 7800, Train Loss: 2.2048778533935547, Val Loss: 2.242316484451294
    Step: 7900, Train Loss: 2.1915619373321533, Val Loss: 2.246105432510376
    Step: 8000, Train Loss: 2.2090227603912354, Val Loss: 2.2380495071411133
    Step: 8100, Train Loss: 2.2073724269866943, Val Loss: 2.2396249771118164
    Step: 8200, Train Loss: 2.199066400527954, Val Loss: 2.246986150741577
    Step: 8300, Train Loss: 2.20217227935791, Val Loss: 2.2386043071746826
    Step: 8400, Train Loss: 2.214346408843994, Val Loss: 2.2508885860443115
    Step: 8500, Train Loss: 2.1967275142669678, Val Loss: 2.24548077583313
    Step: 8600, Train Loss: 2.1883537769317627, Val Loss: 2.242337226867676
    Step: 8700, Train Loss: 2.201338768005371, Val Loss: 2.23762583732605
    Step: 8800, Train Loss: 2.1875431537628174, Val Loss: 2.244039297103882
    Step: 8900, Train Loss: 2.189452886581421, Val Loss: 2.245622158050537
    Step: 9000, Train Loss: 2.190303087234497, Val Loss: 2.2310354709625244
    Step: 9100, Train Loss: 2.1938412189483643, Val Loss: 2.2296013832092285
    Step: 9200, Train Loss: 2.1910276412963867, Val Loss: 2.243750810623169
    Step: 9300, Train Loss: 2.175438642501831, Val Loss: 2.223936080932617
    Step: 9400, Train Loss: 2.186581611633301, Val Loss: 2.2453250885009766
    Step: 9500, Train Loss: 2.1792337894439697, Val Loss: 2.2210822105407715
    Step: 9600, Train Loss: 2.1868937015533447, Val Loss: 2.2494568824768066
    Step: 9700, Train Loss: 2.1882855892181396, Val Loss: 2.237379550933838
    Step: 9800, Train Loss: 2.1887569427490234, Val Loss: 2.241284132003784
    Step: 9900, Train Loss: 2.178579568862915, Val Loss: 2.2159388065338135
    Step: 9999, Train Loss: 2.1877453327178955, Val Loss: 2.239150047302246


And we got further improvement:

`Step: 9999, Train Loss: 2.1877453327178955, Val Loss: 2.239150047302246`.

Lets generate some text and see what we got:


```python
# Example - generate new tokens
idx = torch.zeros((1,1), dtype=torch.long).to(device)
new_tokens = model.generate(idx, 500)[0].tolist()

# decode the generated tokens
gen_txt = decode(new_tokens)
print(gen_txt)
```

    
    He thage vireshe me he an ight innshy mord I that it making Efe,
    Wouch thee moy lact weed her
    Whaters berends, ratis mut is't on a koner is you tersthe whas on ipleles arom's
    Ade aruter aye thy tho-t vis here to' tio os ne of mignce, my
    Thyou agegme.
    
    JUMENLONTHERIO:
    Throu for nojay co shave loow thom his of this Pur Of Hea
    us, will cand fous and you ut, cie.
    
    LICHAMANUS:
    Hom then yem:
    You wes.
    Hellost, my friese wallyoose
    therk
    eveve, wam,
    
    I IV:
    Whath wrearceallanto gow hat.
    
    MENWARD:
    A's wims


There is still room for improvement, but the model is already generating text with some correct english words (i.e: `he`, `I`, `that`,`making`, `here`)

### 8. The Transformer Model

Now, that we have implemented the multi-head attention module, we can build the transformer model. The strength of the transformer model is that it can learn the inter-relations between tokens, even if they are far apart and complex. 

Lets look at the transformer block, as described in the paper:

![transformer-model](/assets/images/gpt/transformer_model.png)

We are now going to break down the transformer model into its components:

#### **Masked Multi-Head Attention**
This is the multi-head attention we have implemented before.

#### **Layer Normalization**
This is a normalization technique similar to batch normalization, but instead of normalizing over the batch dimension, we normalize over the sequence dimension. This is important because we want to keep the inter-relations in the same sequence, and keep the different sequences separate.

In pytorch we can use `nn.LayerNorm` to implement this.

*Note:* In contrast to the paper, it is now common to use layer normaliation in the input of the multi-head attention module, and not in the output.

#### **Skip Connections**
As our network gets deeper, the gradients can become very small, and the training can be very difficult. Skip connections are simply the addition of the input to the output of a layer. This allows the gradients to flow directly to the input of the layer, and at the same time also modify the weights of the layer.

#### **Feed Forward Network**
The feed forward network is a simple linear layer followed by non-linearity. This helps the model learn more complex patterns in the data.

#### **Projection Layer**
The projection layer is at the end of the transformer block, and is used to project the output of the transformer block back to the original size of the embeddings. It is also learnable.

#### **Dropout**
Dropout is a regularization technique used to prevent overfitting. It works by randomly setting a fraction of the input to zero. In pytorch we can use `nn.Dropout` to implement this.


Putting all of this together we get the transformer block:


```python
class FeedForward(nn.Module):
    """A simple linear layer followed by a ReLU activation"""
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(0.2)
        )
        
    def forward(self, x):
        return self.net(x)
```


```python
class TransformerBlock(nn.Module):
    """A single transformer block"""
    def __init__(self, n_embd, n_heads, block_size):
        super().__init__()
        head_size = n_embd // n_heads
        self.att = MultiHeadAttention(n_heads, n_embd, head_size, block_size)
        self.ffwd = FeedForward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.att(self.ln1(x))   # layer norm and skip connection
        x = x + self.ffwd(self.ln2(x))  # layer norm and skip connection
        return x
```

Now lets build the BigramTransformer model, and train it.


```python
class BigramLanguageModelTransformer(nn.Module):
    def __init__(self, vocab_size, n_embd=32, n_heads=4, n_blocks=4):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.pos_embedding_table = nn.Embedding(block_size, n_embd)
        self.block_list = nn.ModuleList([TransformerBlock(n_embd, n_heads, block_size) for _ in range(n_blocks)])
        self.blocks = nn.Sequential(*self.block_list)
        self.ln = nn.LayerNorm(n_embd)  # layer norm before the final head
        self.head = nn.Linear(n_embd, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        token_embd = self.token_embedding_table(idx) # (B,T,n_embd)
        pos_embd = self.pos_embedding_table(torch.arange(T, device=device)) # (T,n_embd)

        x = token_embd + pos_embd   # adding positional embedding to token embedding
        x = self.blocks(x) # apply transformer blocks (B,T,n_embd)
        x = self.ln(x) # layer norm
        logits = self.head(x) # (B,T,vocab_size)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            loss = F.cross_entropy(logits.view(B*T, C),  targets.view(B*T))

        return logits ,loss
    
    def generate(self, idx, max_new_tokens):
        """ Generate new tokens, given current context.
            In a simple B-gram model, we just need to look up the logits for the next token.
            In more complex models, we are also intrested in the history of the tokens. In other
            words, a larger context.

            Args:
                idx (Tensor): Initial context
                max_new_tokens (int): Maximum number of new tokens to generate.
            Returns:
                Tensor: Generated tokens

        """

        # idx is a tensor of shape (B, T)
        idx.to(device)
        for i in range(max_new_tokens):
            idx_cond = idx[:, -block_size:] # only consider the last block_size tokens
            # get the prediction for the current context
            logits, loss = self.forward(idx_cond) # logits shape B, T, C
            # In B-gram model, focus only on the last time step.
            logits = logits[:, -1, :]   
            # convert logits to probabilities
            probs = F.softmax(logits, dim=-1)
            # sample the next token from the distribution
            next_token = torch.multinomial(probs, num_samples=1).to(device)
            idx = torch.cat([idx, next_token], dim=1) # shape B, T+1

        return idx
```

Now lets update the hyperparameters and train the model.


```python
# hyper parameters
num_epochs = 10000
batch_size = 32
n_embd = 32
eval_interval = 100
lr = 1e-3
n_heads = 4
n_transformer_blocks = 4

model = BigramLanguageModelTransformer(vocab_size, n_embd, n_heads, n_transformer_blocks)
model = model.to(device)

# pytorch optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

# training loop
for step in range(num_epochs):
    
    if step % eval_interval == 0:
        losses = estimate_loss()
        print(f"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}")

    # sample a batch of data
    xb, yb = get_batch(split="train")
    # forward pass
    logits, loss = model(xb, yb)

    # backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

losses = estimate_loss()
print(f"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}")
```

    Step: 0, Train Loss: 4.290696620941162, Val Loss: 4.296289443969727
    Step: 100, Train Loss: 3.0215697288513184, Val Loss: 3.0487074851989746
    Step: 200, Train Loss: 2.66170597076416, Val Loss: 2.668571710586548
    Step: 300, Train Loss: 2.531599521636963, Val Loss: 2.5354340076446533
    Step: 400, Train Loss: 2.4548087120056152, Val Loss: 2.449009418487549
    Step: 500, Train Loss: 2.4086432456970215, Val Loss: 2.40623140335083
    Step: 600, Train Loss: 2.3734657764434814, Val Loss: 2.37636661529541
    Step: 700, Train Loss: 2.342966079711914, Val Loss: 2.34257173538208
    Step: 800, Train Loss: 2.327794075012207, Val Loss: 2.3352389335632324
    Step: 900, Train Loss: 2.3091390132904053, Val Loss: 2.333873987197876
    Step: 1000, Train Loss: 2.2615866661071777, Val Loss: 2.2774815559387207
    Step: 1100, Train Loss: 2.2580363750457764, Val Loss: 2.2655227184295654
    Step: 1200, Train Loss: 2.2333953380584717, Val Loss: 2.2473042011260986
    Step: 1300, Train Loss: 2.2223074436187744, Val Loss: 2.2426459789276123
    Step: 1400, Train Loss: 2.2112908363342285, Val Loss: 2.2319555282592773
    Step: 1500, Train Loss: 2.1781005859375, Val Loss: 2.2212538719177246
    Step: 1600, Train Loss: 2.177574396133423, Val Loss: 2.2048816680908203
    Step: 1700, Train Loss: 2.1675424575805664, Val Loss: 2.192692518234253
    Step: 1800, Train Loss: 2.1503896713256836, Val Loss: 2.200355052947998
    Step: 1900, Train Loss: 2.1887078285217285, Val Loss: 2.2103335857391357
    Step: 2000, Train Loss: 2.1397993564605713, Val Loss: 2.180344581604004
    Step: 2100, Train Loss: 2.1217267513275146, Val Loss: 2.1652352809906006
    Step: 2200, Train Loss: 2.1240415573120117, Val Loss: 2.1642942428588867
    Step: 2300, Train Loss: 2.1162383556365967, Val Loss: 2.145756959915161
    Step: 2400, Train Loss: 2.1107726097106934, Val Loss: 2.1441304683685303
    Step: 2500, Train Loss: 2.102898120880127, Val Loss: 2.1437971591949463
    Step: 2600, Train Loss: 2.0784289836883545, Val Loss: 2.133171796798706
    Step: 2700, Train Loss: 2.08516526222229, Val Loss: 2.1497011184692383
    Step: 2800, Train Loss: 2.075202703475952, Val Loss: 2.1368277072906494
    Step: 2900, Train Loss: 2.071418285369873, Val Loss: 2.1315712928771973
    Step: 3000, Train Loss: 2.062307119369507, Val Loss: 2.1246488094329834
    Step: 3100, Train Loss: 2.054274320602417, Val Loss: 2.127197742462158
    Step: 3200, Train Loss: 2.072622776031494, Val Loss: 2.1164743900299072
    Step: 3300, Train Loss: 2.046212673187256, Val Loss: 2.1100220680236816
    Step: 3400, Train Loss: 2.0335614681243896, Val Loss: 2.1183602809906006
    Step: 3500, Train Loss: 2.038421869277954, Val Loss: 2.1065621376037598
    Step: 3600, Train Loss: 2.0210046768188477, Val Loss: 2.096266984939575
    Step: 3700, Train Loss: 2.0314407348632812, Val Loss: 2.0902771949768066
    Step: 3800, Train Loss: 2.031346082687378, Val Loss: 2.1025235652923584
    Step: 3900, Train Loss: 2.0234203338623047, Val Loss: 2.102994680404663
    Step: 4000, Train Loss: 2.0214662551879883, Val Loss: 2.0919201374053955
    Step: 4100, Train Loss: 2.018073081970215, Val Loss: 2.0899453163146973
    Step: 4200, Train Loss: 2.0057713985443115, Val Loss: 2.085452079772949
    Step: 4300, Train Loss: 2.016714572906494, Val Loss: 2.08943772315979
    Step: 4400, Train Loss: 1.99434232711792, Val Loss: 2.0827245712280273
    Step: 4500, Train Loss: 2.0090465545654297, Val Loss: 2.0688529014587402
    Step: 4600, Train Loss: 1.9968235492706299, Val Loss: 2.092362880706787
    Step: 4700, Train Loss: 1.9839783906936646, Val Loss: 2.074103593826294
    Step: 4800, Train Loss: 1.9929178953170776, Val Loss: 2.076368808746338
    Step: 4900, Train Loss: 1.983061671257019, Val Loss: 2.0836009979248047
    Step: 5000, Train Loss: 1.988928198814392, Val Loss: 2.0740392208099365
    Step: 5100, Train Loss: 1.9676650762557983, Val Loss: 2.0722556114196777
    Step: 5200, Train Loss: 1.975730538368225, Val Loss: 2.067152500152588
    Step: 5300, Train Loss: 1.9784895181655884, Val Loss: 2.069321393966675
    Step: 5400, Train Loss: 1.9540681838989258, Val Loss: 2.045891523361206
    Step: 5500, Train Loss: 1.9688762426376343, Val Loss: 2.067540407180786
    Step: 5600, Train Loss: 1.9732582569122314, Val Loss: 2.048455238342285
    Step: 5700, Train Loss: 1.959861397743225, Val Loss: 2.0542256832122803
    Step: 5800, Train Loss: 1.9473516941070557, Val Loss: 2.062727451324463
    Step: 5900, Train Loss: 1.9480491876602173, Val Loss: 2.0613183975219727
    Step: 6000, Train Loss: 1.9561805725097656, Val Loss: 2.0455307960510254
    Step: 6100, Train Loss: 1.9582339525222778, Val Loss: 2.059088706970215
    Step: 6200, Train Loss: 1.9598448276519775, Val Loss: 2.0574791431427
    Step: 6300, Train Loss: 1.9343398809432983, Val Loss: 2.0569777488708496
    Step: 6400, Train Loss: 1.9483935832977295, Val Loss: 2.0588653087615967
    Step: 6500, Train Loss: 1.9518325328826904, Val Loss: 2.053361654281616
    Step: 6600, Train Loss: 1.9455862045288086, Val Loss: 2.0520095825195312
    Step: 6700, Train Loss: 1.9489837884902954, Val Loss: 2.03539776802063
    Step: 6800, Train Loss: 1.944834589958191, Val Loss: 2.03824782371521
    Step: 6900, Train Loss: 1.9268341064453125, Val Loss: 2.028163433074951
    Step: 7000, Train Loss: 1.9509097337722778, Val Loss: 2.0318753719329834
    Step: 7100, Train Loss: 1.933567762374878, Val Loss: 2.0335607528686523
    Step: 7200, Train Loss: 1.9340269565582275, Val Loss: 2.0310518741607666
    Step: 7300, Train Loss: 1.927329421043396, Val Loss: 2.0342421531677246
    Step: 7400, Train Loss: 1.9367098808288574, Val Loss: 2.0319621562957764
    Step: 7500, Train Loss: 1.9291311502456665, Val Loss: 2.044363021850586
    Step: 7600, Train Loss: 1.9203606843948364, Val Loss: 2.027949810028076
    Step: 7700, Train Loss: 1.9165693521499634, Val Loss: 2.033198356628418
    Step: 7800, Train Loss: 1.9234026670455933, Val Loss: 2.037635087966919
    Step: 7900, Train Loss: 1.914462685585022, Val Loss: 2.0147457122802734
    Step: 8000, Train Loss: 1.914597511291504, Val Loss: 2.0253593921661377
    Step: 8100, Train Loss: 1.9168471097946167, Val Loss: 2.0470473766326904
    Step: 8200, Train Loss: 1.914642333984375, Val Loss: 2.0462846755981445
    Step: 8300, Train Loss: 1.9013055562973022, Val Loss: 2.027458906173706
    Step: 8400, Train Loss: 1.9135873317718506, Val Loss: 2.024125576019287
    Step: 8500, Train Loss: 1.9073246717453003, Val Loss: 2.038285493850708
    Step: 8600, Train Loss: 1.9215093851089478, Val Loss: 2.0202338695526123
    Step: 8700, Train Loss: 1.908416748046875, Val Loss: 2.0356903076171875
    Step: 8800, Train Loss: 1.9184378385543823, Val Loss: 2.029233932495117
    Step: 8900, Train Loss: 1.9092414379119873, Val Loss: 2.0249972343444824
    Step: 9000, Train Loss: 1.8992440700531006, Val Loss: 2.018636703491211
    Step: 9100, Train Loss: 1.8967719078063965, Val Loss: 2.0260958671569824
    Step: 9200, Train Loss: 1.8977339267730713, Val Loss: 2.018186092376709
    Step: 9300, Train Loss: 1.8972232341766357, Val Loss: 2.0097014904022217
    Step: 9400, Train Loss: 1.8980292081832886, Val Loss: 2.0152320861816406
    Step: 9500, Train Loss: 1.8932551145553589, Val Loss: 2.0107460021972656
    Step: 9600, Train Loss: 1.9113399982452393, Val Loss: 2.0229384899139404
    Step: 9700, Train Loss: 1.8946348428726196, Val Loss: 2.0148062705993652
    Step: 9800, Train Loss: 1.887571096420288, Val Loss: 2.0086569786071777
    Step: 9900, Train Loss: 1.886356234550476, Val Loss: 2.0157370567321777
    Step: 9999, Train Loss: 1.8907207250595093, Val Loss: 2.0092897415161133


We got further improvement, and the loss dropped to:

`Step: 9999, Train Loss: 1.8907207250595093, Val Loss: 2.0092897415161133`

Lets generate some text and see what we got:


```python
# generate text from the model
context = torch.zeros((1,1), dtype=torch.long).to(device)
new_tokens = model.generate(context, 500)[0].tolist()

# decode the generated tokens
gen_txt = decode(new_tokens)
print(gen_txt)
```

    
    Sit unwhs-as of ungenizee to belife mises
    At draws by troplince, glood;
    geill ave eveen
    I gracly bit
    recommentated,
    Breenness: and to villoumply,
    I to prick'd, a should despirs.
    Telld; bark'?
    
    Seconace.
    That with him for to like displelease I wrest again,
    The shall on the beid I brotitus:
    Thhou go that het good thus
    The postas?
    
    Fit;
    Will aid that deport the grough
    Beg the Cevery copders!
    
    BRIONGHARET:
    Thy, but age!
    
    MENENY Mise or beeen is and him wash Purpt must my wex tried his how theseth sw


This looks much better. The words look a lot more like english words, and the model has learned to generate text that looks like Shakespeare.
This not perfect, but it is a good result for a simple transformer model.



