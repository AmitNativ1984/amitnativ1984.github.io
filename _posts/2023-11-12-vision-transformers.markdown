---
layout: posts
title:  "Vision Transformers and Attention"
tagline: "Understanding vision transformers and attention mechanism"
date:   2023-11-12
categories: COMPUTER-VISION
permalink: /:categories/:title
header:
  overlay_image: /assets/images/transformers/transformer-robot.jpg
  overlay_filter: 0.3
toc: true
created: true
classes: wide
---

# **Vision Transformers (ViT) Explained**

Vision and language are two big domains in machine learning. In the lasten few years, we have seen a lot of progress in the field of NLP, but the same cannot be said for computer vision. Most of the computer vision tasks are still dominated by Convolutional Neural Networks (CNNs). But recently, there has been a lot of research in the field of vision transformers, and they have shown promising results. In this article, we will explore the vision transformers and see how they work.

This post follows the paper: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. A seminal work that first introduced how vision transformers can be used for image classification tasks.

Before we dive into the details, lets first talk about inductive bias.

## Inductive Bias
Inductive bias is a set of assumptions that the learning algorithm uses to predict outputs of unseen inputs. For example, in *linear regression*, we assume the data was generated via a linear process with additive gaussian noise. Similarly, in *Convolutional Neural Networks (CNNs)* we assume that the data is generated by a translation invariant process, and that a strong connection exists between adjacent pixels or image patches. This is the reason why CNNs are good at capturing spatial information. This means that CNNs have a strong inductive bias towards spatial information, but are not good at capturing long-range dependencies between pixels in the image. This is where transformers come in. As we see later, transformers permute the input data and capture long-range dependencies between the input data as well. This is the reason why transformers are good at capturing global information. 

## How Vision Transformers Work
> 1. Split the image into patches
> 2. Flatten the patches
> 3. Project the flattened patches into a lower dimensional linear embedding.
> 4. Add positional embeddings.
> 5. Feed the sequence of embeddings into a transformer encoder.
> 6. Train the model with image labels and classification loss.

![ViT](/assets/images/transformers/ViT.gif){:width="1000"}

As can be seen from the image above, the input image is split into patches, breaking spatial inductive bias of CNNs. The patches are then flattened and projected into a lower dimensional embedding. Positional embeddings are added to the flattened patches.

The image patches are similar to tokens in NLP, and the encoder block is identical to the original transformer block proposed in the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. 

The only major difference is that in the original transformer used for NLP the positional embeddings are use pre-defined sinusoidal embeddings, whereas in vision transformers the positional embeddings are learnable vectors. Lets dive into the details of each step, before going into the transformer encoder block.

## Linear Projection
In this step, each image patch of size $M\times N$ if first flattened into a vector. This vector, is then projected through a projection matrix $E$ to get the patch embedding. The projection matrix $E$ is learnable, and is shared across all the patches. 

![linear-projection](/assets/images/transformers/linear-projection.jpeg){:width="800"}

The outcome is that each of the image patches in the image, now has a corresponding embedding $\mathbf{z}_i \in \mathbb{R}^d$.


## Classification Token
In addition to the patch embeddings, another vector of size $d$ is used to represent the class. This vector is called the classification token, and is also of size $d$. It is is added to the sequence of patch embeddings, and is the first in the sequence.


A learnable classification token $\mathbf{c} \in \mathbb{R}^d$ is also added to the sequence of patch embeddings. This classification token is used to predict the class of the image. The classification token is added to the sequence of patch embeddings, and is the first token in the sequence.

## Positional Embeddings
Transformers do not have any default mechanism to consider the "order" of the tokens or patches. However, information about the order is essential! For example, in NLP the order of words is important. For example: "I like apples" and "apples like I" have different meanings. The same is true for images. Lets look at the picture of the puzzle below:

![puzzle](/assets/images/transformers/puzzle.webp){: width="650"}


The pieces of the puzzle are the patches. To get the correct image, we need to arrange the pieces in the correct order. This is the reason why we need positional embeddings. For vision transformers, the positional embeddings are learnable vector, with the same dimensionality as the path embeddings. The positional embeddings are just **added** to the patch embeddings before feeding them into the transformer encoder.

As we will see in the implementation, during training these embeddings converge into vector spaces the show high similarity between neighboring patches. 

The learned positional embedding is learned by the model, and each patch embedding is added to the positional embedding before feeding it into the transformer encoder.

![position-encoding-2.png](/assets/images/transformers/positional-encoding-2.png){:width="600"}


After training is done, we can check the similarity between the positional embeddings of the patches. The image below shows the cosine similarity between the positional embeddings of the patches. It shows that the transformer learns the inductive bias of the image, and learns to place similar patches close to each other.

![positional-encoding](/assets/images/transformers/positional-encoding.png){:width="600"}

# Attention
Below is a diagram of the transformer encoder. In a complete transformer model, there are $L$ such encoder blocks stacked on top of each other. The output of the last encoder block is then fed into a linear layer to get the final output.

The first step of the encoder, is a normalization layer. This layer is important to improve training, decrease the variance of the input data, and improve the stability of the model. 
The output of the normalization layer is a **Multi-Head Attention** layer, or **MHA**. Before we dive into the details of MHA, lets first talk about attention.

![transfomer-block](/assets/images/transformers/the-transformer-block-vit.png){:height="400"}

## Self-Attention
Attention is a mechanism that allows the model to focus on important parts of the input. Self-attention was first introduced in the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. Self-attention is a mechanism that allows the model to better learn the relationship between different parts of the inputs. In the case of ViT, the input is a sequence of patch embeddings. The way the patches relate to each other and also to the entire image, have huge affect on the final output of the model. 

Here is how to calculate self-attention: Lets define Three matrices, called Query, Key, and Value. These matrices are calculated by multiplying the input sequence of patch embeddings with three learnable weight matrices.

$$
\begin{aligned}
\mathbf{Q} &= \mathbf{Z}\mathbf{W}^Q \\
\mathbf{K} &= \mathbf{Z}\mathbf{W}^K \\
\mathbf{V} &= \mathbf{Z}\mathbf{W}^V \\
\end{aligned}
$$

Where $\mathbf{Z} \in \mathbb{R}^{p \times d}$ is the input sequence of patch embeddings, all stacked on top of each other,

$$
\begin{aligned}
\mathbf{Z} &= 
    \begin{bmatrix} -z_1-\\
                    -z_2-\\
                    -z_3- \\
                    ... \\
                    -z_p- \\
\end{bmatrix}_{p\times d}
\end{aligned}
$$

and $\mathbf{W}^Q, \mathbf{W}^K, \mathbf{W}^V \in \mathbb{R}^{d \times d}$ are learnable weight matrices. 

![q-k-v](/assets/images/transformers/q-k-v.jpeg){:width="800"}

In a very simplistic way, embeddings that have high information about the input, will store a lot of information, and will relate well to other embeddings with information about the input.

Therefore, we can take the dot product of the query $\mathbf{Q}$ matrix with the key $\mathbf{K}$ matrix. This will give the similarity between the embeddings, and then multiply the result with the value $\mathbf{V}$ matrix. The output of this operation is the output of the self-attention layer.

The output of the self-attention layer is calculated as follows:

$$
\begin{aligned}
\mathbf{A} &= \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}}\right)\mathbf{V}
\end{aligned}
$$

Notice the projection (dot product) between $\mathbf{Q}$ and $\mathbf{K}^T$. This is the similarity between the embeddings. The softmax function is used to normalize the similarity scores, and the output is multiplied by the value matrix $\mathbf{V}$.

![self-attention](/assets/images/transformers/attention-mechanism-step-2.png){:width="800"}


### Multi-Head Attention
Multi-head attention is nothing more than repeating the self-attention layer multiple times, learning different relationships between patches by adding more learnable $\mathbf{K},\mathbf{Q},\mathbf{V}$ matrices. The output of the multi-head attention layer is then concatenated and fed into a linear layer to get the final output.

![multi-head-attention](/assets/images/transformers/multi-head-self-attention-block-diagram.png){:width="800"}


The output of the attention layer is then fed into the feed-forward network, which is a simple MLP. Finally, the ouput of the encoder is fed into a classification layer to get the final output.